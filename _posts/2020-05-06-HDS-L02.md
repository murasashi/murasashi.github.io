---
title: Good event of LASSO
author: Murasashi
date: 2020-05-06 16:00:00 +0400
categories: [Statistics] 
tags: [Lasso]
---

# High-Dimensional Model Lecture 02

## Main Reference

- Sara van der Geer. P.107. Lemma 6.3


## Structure 

## The Lasso program

Given a data set $D_{[t]}=\{(X_{s}, Y_{s}): s\in [t]\}$, the __LASSO estimator__  $\theta_{t}$ w.r.t. regularization parameter $\lambda_{t}$ is defined as 

$$\hat{\theta}_{t}\equiv\arg\min_{\theta}\bigg\{t^{-1}\|Y_{[t]}-X_{[t]}\theta\|_2^2+\lambda_{t}\|\theta\|_1\bigg\}.\hspace{20pt}(\clubsuit)$$

For convenience, we define the __LASSO objective function__ as

$$f_{t}(\theta; \lambda_{t}) = t^{-1}\|Y_{[t]}-X_{[t]}\theta\|_2^2+\lambda_{t}\|\theta\|_1.\hspace{20pt}(\heartsuit)$$


## The Good Event

$$G(\lambda_{t}) \equiv\{\frac{4}{t}\|X_{[t]}^\top\epsilon_{[t]}\|_{\infty}\le \lambda_{t}\}$$

## Point 01: Basic inequality (Lemma 6.1)

Use the property of the LASSO minimizer $(\heartsuit)$, one has the basic inequality.
> Lemma 6.1. 
> $$t^{-1}\|X_{[t]}(\hat{\theta}_{t}-\theta_{*})\|_2^2+\lambda_{t}\|\hat{\theta}_{t}\|_1\le 2t^{-1}\epsilon_{[t]}^\top X_{[t]}(\hat{\theta}_{t}-\theta_{*})+\lambda_{t}\|\theta_{*}\|_1$$

Use Holder inequality, we have

$$t^{-1}\|X_{[t]}(\hat{\theta}_{t}-\theta_{*})\|_2^2+\lambda_{t}\|\hat{\theta}_{t}\|_1\le 2t^{-1}\|\epsilon_{[t]}^\top X_{[t]}\|_{\infty}\|\hat{\theta}_{t}-\theta_{*}\|_{1}+\lambda_{t}\|\theta_{*}\|_1$$


## Point 02: Good event + Sparsity (Lemma 6.3)

>Lemma 6.3. Given the good event $G(\lambda_t)$, sparsity gives
> 
> $$2t^{-1}\|X_{[t]}(\hat{\theta}_{t}-\theta_{*})\|_2^2+\lambda_{t}\|\hat{\theta}_{S_{0}^{c}}\|_1\le 3\lambda_{t}\|\hat{\theta}_{t, S_{0}}-\theta_{*, S_{0}}\|_1$$


## Point 03: Compatability Condition

This gives a version of __oracle inequality__.


## Point 04: LASSO stationary condition

For some subgradient $\hat{z}\in\partial\|\hat{\theta}_{t,\lambda_{t}}\|_1$, the first-order stationarity conditions for an optimum gives
$$t^{-1}X_{[t]}^\top (X_{[t]}\hat{\theta}_{t, \lambda_{t}}-Y_{[t]})+\lambda \hat{z}=0$$

## Summary

To read more on GLM LASSO

- Tuning parameter calibration for â„“1-regularized logistic regression
