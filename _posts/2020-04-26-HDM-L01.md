---
title: Oracle Inequalities for High-dimensional Prediction (Technical Review)
author: Murasashi
date: 2020-04-26 16:00:00 +0400
categories: [Statistics] 
tags: [oracle, penalty]
---

# High-Dimensional Model Lecture 01

## Main Reference

- Oracle Inequalities for High-dimensional Prediction



## Structure

1. Estimators
2. Penalty bound.

## Point 01: Estimators


### Estimators

$$
\hat{\beta}^{\lambda} 
\in \arg\min_{\beta \in \mathbb{R}^{p}}
\big\{
g(\|Y-X\beta\|_2^2)
+
\sum_{j=1}^{k}\lambda_{j}\|M_{j}\beta\|_{q_j}
\big\} ~~~~~(\clubsuit)
$$

- $(\clubsuit)$ has $k$ different penalties, each are described by tuning parameter  $\lambda$ and structure parameter $M$ with norm $\|\cdot\|_{q_j}$.

- __link function__ $g(\cdot)$ needs to have some conditions on convexity, to include the case $g(x)=x$ (LASSO) and $f(x) = \sqrt{x}$ (Square-root Lasso).

- __Penalties__  can have $k$ different kinds, although we only need $k=1$ to tame  (LASSO) and (Square-root Lasso).
 
## Point 02: On theoretical choice of tuning parameter $\lambda$


### Theorem 2.1-Equation (5)
The main equation (5) is what we adaptive to use, cited the version of $(k=1, M_1=I_p, q_1 = 1)$ here as

$$
n^{-1}\|X(\beta^{*}-\bar{\beta})\|^2_2
\le 
2n^{-1}\|X^\top\epsilon\|^{*}_{1}\|\beta^{*}\|_1,
$$
which is what we get at standard LASSO argument (???).

### Lemma 2.1

Fix a constant $c\in(0,\infty)$. With probability one, there is a tuning parameter $\lambda(c)$ such that 

$$
\frac{\lambda}{2g^{'}(\|Y-X\hat{\beta}^{\lambda}\|_2^2)}= c\|X^\top\epsilon\|_1^{*}. ~~~~~~~~~(\diamondsuit)
$$

- $(\diamondsuit)$ indicates the _theoretical_ choice of tuning parameter $\lambda$, which depends on the __empirical process__ part $\|X^\top\epsilon\|_1^{*}$ and the __model risk score (a proper name?)__ part $g^{'}(\|Y-X\hat{\beta}^{\lambda}\|_2^2)$.


## Point 03: Argument toward Theorem 2.1

### Elements

- Objective function

$$f(\beta) \equiv g(\|Y-X\beta\|_2^2)+\lambda\|\beta\|_1.$$

- Subdifferentials (to check stationary optimality condition)

$$\partial f(\beta)|_{\beta = \hat{\beta}^{\lambda}}=-2g^{'}(\|Y-X\beta\|_2^2)(X^\top(Y-X\hat{\beta}^{\lambda})) + \lambda \cdot \partial \lambda\|\beta\|_1|_{\beta = \hat{\beta}^{\lambda}}$$

- Subgradient $\kappa \in \lambda \cdot \partial \lambda\|\beta\|_1|_{\beta = \hat{\beta}^{\lambda}}$ satisfies

$$(\forall \beta \in \mathbb{R}^{p}) \hspace{20pt} \|\beta\|_1 \ge \|\hat{\beta}\|_1 + \langle \kappa, \beta-\hat{\beta} \rangle.$$

- Check Stationary optimality condition


## Summary
