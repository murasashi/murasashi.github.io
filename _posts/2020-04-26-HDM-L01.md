---
title: Oracle Inequalities
author: Murasashi
date: 2020-04-26 16:00:00 +0400
categories: [Statistics] 
tags: [oracle, penalty]
---

# High-Dimensional Model Lecture 01

## Main Reference
- Oracle Inequalities for High-dimensional Prediction



## Structure

1. Estimators
2. Penalty bound.

## Point 01: Estimators


### Estimators

$$
\hat{\beta}^{\lambda} 
\in \arg\min_{\beta \in \mathbb{R}^{p}}
\big\{
g(\|Y-X\beta\|_2^2)
+
\sum_{j=1}^{k}\lambda_{j}\|M_{j}\beta\|_{q_j}
\big\} ~~~~~(\clubsuit)
$$

- $(\clubsuit)$ has $k$ different penalties, each are described by tuning parameter  $\lambda$ and structure parameter $M$ with norm $\|\cdot\|_{q_j}$.

- __link function__ $g(\cdot)$ needs to have some conditions on convexity, to include the case $g(x)=x$ (LASSO) and $f(x) = \sqrt{x}$ (Square-root Lasso).

- __Penalties__  can have $k$ different kinds, although we only need $k=1$ to tame  (LASSO) and (Square-root Lasso).
 
## Point 02: On theoretical choice of tuning parameter $\lambda$


### Theorem 2.1-Equation (5)
The main equation (5) is what we adaptive to use, cited the version of $(k=1, M_1=I_p, q_1 = 1)$ here as

$$
n^{-1}\|X(\beta^{*}-\bar{\beta})\|^2_2
\le 
2n^{-1}\|X^\top\epsilon\|^{*}_{1}\|\beta^{*}\|_1,
$$
which is what we get at standard LASSO argument (???).

### Lemma 2.1

Fix a constant $c\in(0,\infty)$. With probability one, there is a tuning parameter $\lambda(c)$ such that 

$$
\frac{\lambda}{2g^{'}(\|Y-X\hat{\beta}^{\lambda}\|_2^2)}= c\|X^\top\epsilon\|_1^{*}. ~~~~~~~~~(\diamondsuit)
$$

- $(\diamondsuit)$ indicates the _theoretical_ choice of tuning parameter $\lambda$, which depends on the __empirical process__ part $\|X^\top\epsilon\|_1^{*}$ and the __model risk score (a proper name?)__ part $g^{'}(\|Y-X\hat{\beta}^{\lambda}\|_2^2)$.


## Point 03: Argument toward Theorem 2.1

### Elements

- Objective function

$$f(\beta) \equiv g(\|Y-X\beta\|_2^2)+\lambda\|\beta\|_1.$$

- Subdifferentials (to check stationary optimality condition)

$$\partial f(\beta)|_{\beta = \hat{\beta}^{\lambda}}=-2g^{'}(\|Y-X\beta\|_2^2)(X^\top(Y-X\hat{\beta}^{\lambda})) + \lambda \cdot [\partial\|\beta\|_1]_{\beta = \hat{\beta}^{\lambda}}$$

- The first term is handled by __model and noise__

- The second term is handled by picking a __Subgradient__ $\kappa$  from the subdifferential $[\partial \|\beta\|_1]_{\beta =\hat{\beta}^{\lambda}}$, which by its definition satisfies

$$(\forall \beta \in \mathbb{R}^{p}) \hspace{20pt} \|\beta\|_1 \ge \|\hat{\beta}\|_1 + \langle \kappa, \beta-\hat{\beta} \rangle.$$

> The key is to use the implied inequality $\langle \kappa, \beta-\hat{\beta} \rangle \le \|\beta\|_1-\|\hat{\beta}\|_1$, and that's why we only can get 1-dimensional result.


- Check Stationary optimality condition: $\mathbf{0}_{p} \in \partial f(\beta)|_{\beta = \hat{\beta}^{\lambda}}$ to get __estimation equation system__--there is a $\lambda$ such that:

$$
\mathbf{0}_{p} = -2g^{'}(\|Y-X\beta\|_2^2)(X^\top(Y-X\hat{\beta}^{\lambda})) + \lambda \cdot \kappa
$$

 for a subgradient $\kappa \in 
[\partial\|\beta\|_1]_{\beta = \hat{\beta}^{\lambda}}$ (which is a p-dimensional vector). In general, the form of $\kappa$ is untraceable and we work on its consequence.

- We working on the consequence of the above EPS in 1-dimension:

$$
0 = \langle \mathbf{0}_{p} ,  \beta-\hat{\beta} \rangle,
$$
which involves algebras on (1) $\langle (X^\top(Y-X\hat{\beta}^{\lambda})), \beta-\hat{\beta} \rangle$ and  (2) $\langle \kappa, \beta-\hat{\beta} \rangle$.

1. Reduction on the first part is from __statistical model assumptions__ $Y = X\beta + \epsilon$. To gain further insight, let $f_{\beta}(X) = X\beta$ denote the model function w.r.t. _parameter_ $\beta$. Then by direct algebra, one has 

$$
(1) = \|f_{\beta^*}(X)-f_{\hat{\beta}^{\lambda}}(X)\|_2^2+ \langle f_{\beta^*}(X)-f_{\hat{\beta}^{\lambda}}(X), f_{\beta}(X)-f_{\beta^*}(X)\rangle + \langle \epsilon, f_{\beta}(X)-f_{\hat{\beta}^{\lambda}}(X) \rangle
$$

- Use Cauchy-inequality, the second term (1)-(ii), for any $u>0$, has a lower bound in the form 

$$
(1)-(ii) \ge -u  \|f_{\beta^*}(X)-f_{\hat{\beta}^{\lambda}}(X)\|_2^2 - 4u^{-1} \|f_{\beta}(X)-f_{\hat{\beta}^{\lambda}}(X)\|_2^2
$$



- Another key is to handle __the part (proper name?)__

$$ \langle \epsilon, X(\hat{\beta}^{\lambda}-\beta)\rangle= \langle X^\top \epsilon, \hat{\beta}^{\lambda}-\beta\rangle \le \|X^\top\epsilon\|_{\infty}\|\hat{\beta}^{\lambda}-\beta\|_1$$


## More on Lemma 2.1

> Can we remove the requirement on prior knowledge about __response variance__?

The insight from lemma 2.1 on the tuning parameter $\lambda$ is to note $g^{'}(x) = 1$ and $1/(2\sqrt{x})$ for LASSO and Square-root-LASSO. In $L_1$ structure condition, for a given $c$, it should satisfies

$$
c\|X^\top \epsilon\|_{\infty} = \frac{\lambda}{2g^{'}(\|Y-X\hat{\beta}^{\lambda}\|_2^2)}=\begin{cases} \frac{\lambda}{2} &\text{for LASSO}\\ \frac{\lambda}{\|Y-X\hat{\beta}^{\lambda}\|_2^2}&\text{for Square-Root-LASSO}\end{cases}.
$$


## Summary

